{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utah Home Value Predictor\n",
    "This is a regression ML project that inputs an image of a house in the Wasatch Front and outputs an estimated home value as of 2023.\n",
    "The training data is based on assessor-provided images of single-family homes in Davis County, UT.\n",
    "Test data should be valid for homes in non-rural regions of Weber, Davis, Salt Lake, and Utah County, UT, but there may be slight variations due to location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-ML imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dtype = {\n",
    "  \"PARCEL ID\": str,\n",
    "  \"PARCEL ZIP CODE\": str\n",
    "}\n",
    "dtype_main = {\n",
    "  \"Parcel ID\": str,\n",
    "  \"Assessed Value\": np.float32\n",
    "}\n",
    "allowed_prop_types = ['Residential']  # allowed property types on the Davis County parcel system\n",
    "blacklist_parcels = [  # parcels with invalid images or too much foliage blocking the house\n",
    "  '010450014',\n",
    "  '012590319',\n",
    "  '030810075',\n",
    "  '050330045',\n",
    "  '050460021',\n",
    "  '050780007',\n",
    "  '050790009',\n",
    "  '051130041',\n",
    "  '060140083',\n",
    "  '060920057',\n",
    "  '070130004',\n",
    "  '070140068',\n",
    "  '073010128',\n",
    "  '080120020',\n",
    "  '080450003',\n",
    "  '080980011',\n",
    "  '081690013',\n",
    "  '082280005',\n",
    "  '084470305',\n",
    "  '085850315',\n",
    "  '090480038',\n",
    "  '090600006',\n",
    "  '091010078',\n",
    "  '093380401',\n",
    "  '100810002',\n",
    "  '111870221',\n",
    "  '114710015',\n",
    "  '114930076',\n",
    "  '116520007',\n",
    "  '117750010',\n",
    "  '127180016',\n",
    "  '130170032',\n",
    "  '130760106',\n",
    "  '131630030',\n",
    "  '140430050',\n",
    "  '140560003',\n",
    "  '140630013',\n",
    "  '140650050',\n",
    "  '140680016',\n",
    "  '143430059',\n",
    "  '143510048',\n",
    "  '144450025',\n",
    "  '145480125',\n",
    "  '150400102',\n",
    "]\n",
    "random_seed = '70f2f796-b097-4215-b2a8-aa54cd499bbf'  # you can change this but don't be surprised if you get invalid homes that you have to filter through; this seed has been checked up to 1300 parcels\n",
    "train_count = 1000  # how many instances to pull for the training/validation\n",
    "epochs = 5  # how many epochs\n",
    "debug = True  # some print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 (optional): Download/Filter Parcel Master\n",
    "You can download the Parcel Master at https://opendata.gis.utah.gov. There is no direct URL for this so just save it as `./parcel_list/parcels_raw.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parcel_list/parcels_raw.csv', 'r') as f:\n",
    "  df = pd.read_csv(f, dtype=dtype)\n",
    "\n",
    "  # Remove duplicate parcel IDs\n",
    "  df = df.drop_duplicates(subset='PARCEL ID', keep=\"last\")\n",
    "\n",
    "  # Don't include any parcels that are not Private ownership\n",
    "  df = df[df['OWNERSHIP TYPE'] == 'Private']\n",
    "\n",
    "  # Don't include any parcels that have more than 10000 sqm (~2.5 acres) as that will mess up the data collection\n",
    "  df = df[df['Shape__Area'] < 10000]\n",
    "\n",
    "  # Only keep the Parcel IDs and ZIPs as a primary key\n",
    "  df = df[['PARCEL ID','PARCEL ZIP CODE','Shape__Area']]\n",
    "\n",
    "  df.to_csv('parcel_list/parcels_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Get Data from DC Parcel Search\n",
    "This will get the data + images from the Davis County Parcel Search. If you already have files in `images/` and `property_attributes.csv` the download script will not be invoked and only `main_df` needs be instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of filtered parcels\n",
    "with open('parcel_list/parcels_filtered.csv', 'r') as f:\n",
    "  df = pd.read_csv(f, dtype=dtype, index_col='PARCEL ID')\n",
    "\n",
    "# set seed for parcel randomization\n",
    "parcels = df.index.values.tolist()\n",
    "random.seed(random_seed)\n",
    "random.shuffle(parcels)\n",
    "\n",
    "try:\n",
    "  with open('property_attributes.csv', 'r') as f:\n",
    "    # if file already exists, automatically load it\n",
    "    main_df = pd.read_csv(f, dtype=dtype_main, index_col='Parcel ID')\n",
    "except:\n",
    "  # Main Data collection if missing\n",
    "  i = -1\n",
    "  i_success = 0\n",
    "  main_df = pd.DataFrame(columns=[\n",
    "    'Parcel ID',\n",
    "    'Property Type',\n",
    "    'Property Size',\n",
    "    'Year Built',\n",
    "    'Assessed Value'\n",
    "  ], dtype=dtype_main)\n",
    "  main_df = main_df.set_index('Parcel ID')\n",
    "\n",
    "  while i_success < train_count:\n",
    "    i += 1\n",
    "\n",
    "    if debug:\n",
    "      print(f'Trying iloc={i} ({parcels[i]})')\n",
    "\n",
    "    data_core = requests.get(f'https://webportal.daviscountyutah.gov/App/PropertySearch/api/parcel/buildings/{parcels[i]}').json()\n",
    "\n",
    "    # Blank parcel (no property)\n",
    "    if len(data_core) == 0:\n",
    "      if debug:\n",
    "        print(f'Skipped {parcels[i]} (REASON: data)')\n",
    "      continue\n",
    "\n",
    "    data_core = data_core[0]\n",
    "\n",
    "    # Missing physical information\n",
    "    if data_core[\"propertyType\"] not in allowed_prop_types or\\\n",
    "      'bltasYearBuilt' not in data_core or\\\n",
    "      'landGrossAcres' not in data_core or\\\n",
    "      data_core['landGrossAcres'] == 0:\n",
    "      if debug:\n",
    "        print(f'Skipped {parcels[i]} (REASON: data_core)')\n",
    "      continue\n",
    "    \n",
    "    data_value = requests.get(f'https://webportal.daviscountyutah.gov/App/PropertySearch/api/taxrecord/{parcels[i]}').json()\n",
    "\n",
    "    # Missing market value information\n",
    "    if len(data_value) == 0 or\\\n",
    "      'marketImproveValue' not in data_value[0] or\\\n",
    "      'marketLandValue' not in data_value[0] or\\\n",
    "      data_value[0]['marketImproveValue'] == 0 or\\\n",
    "      data_value[0]['marketLandValue'] == 0:\n",
    "      if debug:\n",
    "        print(f'Skipped {parcels[i]} (REASON: data_value)')\n",
    "      continue\n",
    "\n",
    "    data_image = requests.get(f'https://webportal.daviscountyutah.gov/App/PropertySearch/api/parcel/images/{parcels[i]}').json()\n",
    "\n",
    "    # Missing image\n",
    "    if len(data_image) == 0 or parcels[i] in blacklist_parcels:\n",
    "      if debug:\n",
    "        print(f'Skipped {parcels[i]} (REASON: data_image)')\n",
    "      continue\n",
    "    \n",
    "    main_df.loc[parcels[i]] = [\n",
    "      data_core[\"propertyType\"],\n",
    "      data_core[\"landGrossAcres\"],\n",
    "      data_core[\"bltasYearBuilt\"],\n",
    "      data_value[0]['marketImproveValue'] + data_value[0]['marketLandValue']\n",
    "    ]\n",
    "\n",
    "    # export image\n",
    "    raw_image = str.encode(data_image[0].replace('data:image/jpeg;base64,', ''))\n",
    "    \n",
    "    with open(f'images/{parcels[i]}.jpg', 'wb') as b:\n",
    "      b.write(base64.decodebytes(raw_image))\n",
    "    \n",
    "    i_success += 1\n",
    "    \n",
    "  main_df.to_csv('property_attributes.csv')\n",
    "main_df[\"Assessed Value\"] = main_df[\"Assessed Value\"] / 1000000\n",
    "main_df[\"Assessed Value\"] = main_df[\"Assessed Value\"].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all images to be 640x480 ~ 4:3 ratio\n",
    "def convert_image(i):\n",
    "  with open(i, 'rb') as f:\n",
    "    img = Image.open(f)\n",
    "    w = img.width\n",
    "    h = img.height\n",
    "\n",
    "    if w == 640 and h == 480:\n",
    "      return\n",
    "    \n",
    "    # other aspect ratio in landscape mode\n",
    "    elif w >= h * 4/3:\n",
    "      target_w = h * 4/3\n",
    "      biaxial_cropped_w = (w - target_w) / 2\n",
    "\n",
    "      img = img.crop((biaxial_cropped_w, 0, w - biaxial_cropped_w, h))\n",
    "    \n",
    "    # other aspect ratio in portrait mode\n",
    "    else:\n",
    "      target_h = w / (4/3)\n",
    "      biaxial_cropped_h = (h - target_h) / 2\n",
    "\n",
    "      img = img.crop((0, biaxial_cropped_h, w, h - biaxial_cropped_h))\n",
    "    \n",
    "    img = img.resize((640, 480))\n",
    "    img.save(i)\n",
    "\n",
    "for i in os.listdir('images'):\n",
    "  if '.jpg' in i:  # valid image\n",
    "    convert_image(f\"images/{i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Time to Train!\n",
    "Let's train our data now using a simple convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/darrenrs/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "Python(92395) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "100%|██████████| 528M/528M [00:18<00:00, 30.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "# ML imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# function to transform PIL image into np tensor\n",
    "transform_func = transforms.Compose([\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# order of priority: NVIDIA Cuda, Apple MPS, CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "# Dataset class: automatically partitions into 80% train/20% validation\n",
    "class HomeValueDataset(Dataset):\n",
    "  def __init__(self, train=True):\n",
    "    self.imgs = [transform_func(Image.open(f\"images/{x}.jpg\")) for x in main_df.index.values]\n",
    "    self.index_modifier = 0 if train else train_count - (train_count // 5)\n",
    "    self.train = train\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    input = self.imgs[index+self.index_modifier]\n",
    "    output = main_df[\"Assessed Value\"].iloc[index+self.index_modifier]\n",
    "\n",
    "    return input, output\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs) - (len(self.imgs) // 5) if self.train else len(self.imgs) // 5\n",
    "\n",
    "class HomeValueCNN(nn.Module):\n",
    "  def __init__(self, num_outputs=1):\n",
    "    super(HomeValueCNN, self).__init__()\n",
    "    \n",
    "    # Load the VGG16 model pre-trained on ImageNet\n",
    "    vgg = models.vgg16(pretrained=True)\n",
    "    \n",
    "    # Modify the classifier part of VGG to suit 640x480 images\n",
    "    # Remove the last classifier layers and adapt them to regression\n",
    "    \n",
    "    # The VGG architecture for reference\n",
    "    self.features = vgg.features  # Convolutional part remains the same\n",
    "    \n",
    "    # Define custom classifier\n",
    "    self.regressor = nn.Sequential(\n",
    "        nn.Linear(512 * 20 * 15, 4096),  # Adjust input size to match the output from conv layers for 640x480\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096, num_outputs)  # Output size for regression\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # Forward through the VGG feature extractor\n",
    "    x = self.features(x)\n",
    "    \n",
    "    # Flatten the output from the conv layers\n",
    "    x = torch.flatten(x, 1)\n",
    "    \n",
    "    # Forward through the regressor\n",
    "    x = self.regressor(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = HomeValueCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 15.77 GB, other allocations: 82.47 MB, max allowed: 18.13 GB). Tried to allocate 2.34 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m     27\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m   \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m   training_run_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m training_run_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/adam.py:216\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    213\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/adam.py:156\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    146\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    147\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    148\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    160\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    161\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    162\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 15.77 GB, other allocations: 82.47 MB, max allowed: 18.13 GB). Tried to allocate 2.34 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# main train loop\n",
    "def train_model(model, train_loader, val_loader, epochs=epochs):\n",
    "  model.to(device)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    training_run_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      \n",
    "      # Convert labels to float for regression and adjust shape\n",
    "      labels = labels.float().unsqueeze(1)  # Change shape from [batch_size] to [batch_size, 1]\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Backward pass and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      training_run_loss += loss.item() * images.size(0)\n",
    "\n",
    "    training_loss = training_run_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "      for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "      \n",
    "        # Convert labels to float for regression and adjust shape\n",
    "        labels = labels.float().unsqueeze(1)  # Change shape from [batch_size] to [batch_size, 1]\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {training_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Example usage (assuming train_loader and val_loader are defined DataLoader objects)\n",
    "train_dataset = HomeValueDataset(train=True)\n",
    "val_dataset = HomeValueDataset(train=False)\n",
    "\n",
    "# Initialize DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_image(\"test/TestHome700k.jpg\")\n",
    "test_image_tensor = transform_func(Image.open(\"test/TestHome700k.jpg\")).unsqueeze(0)\n",
    "test_image_tensor = test_image_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  prediction = model(test_image_tensor)\n",
    "  predicted_value_700k = prediction.item() * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_image(\"test/TestHome400k.jpg\")\n",
    "test_image_tensor = transform_func(Image.open(\"test/TestHome400k.jpg\")).unsqueeze(0)\n",
    "test_image_tensor = test_image_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  prediction = model(test_image_tensor)\n",
    "  predicted_value_400k = prediction.item() * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_image(\"test/TestHome1.3M.jpg\")\n",
    "test_image_tensor = transform_func(Image.open(\"test/TestHome1.3M.jpg\")).unsqueeze(0)\n",
    "test_image_tensor = test_image_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  prediction = model(test_image_tensor)\n",
    "  predicted_value_1300k = prediction.item() * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228800"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc_input_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
